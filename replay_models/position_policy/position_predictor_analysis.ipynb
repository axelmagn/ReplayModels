{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position Predictor (In Progress)\n",
    "\n",
    "`Given the position of 1,3,5 players + ball have the model try and predict the position of the other player.`\n",
    "\n",
    "The intent of this notebook is to document the analysis of hypotheses related to the development of a position predictor model.  This analysis will be used in the construction and justification of a final model and pipeline.\n",
    "\n",
    "## TODO\n",
    "\n",
    "- [ ] refactor preprocessing to use tensorflow datasets\n",
    "- [ ] add data expansion to preprocessing in a way that is sensible\n",
    "- [ ] test state-of-art for MNIST trained on custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carball\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import itertools\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import sklearn\n",
    "import math\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGS\n",
    "POSITION_COLUMNS = ['pos_x', 'pos_y', 'pos_z']\n",
    "NCOLS = len(POSITION_COLUMNS)\n",
    "RANDOM_SEED = 42\n",
    "TEST_RATIO = 0.2\n",
    "VALIDATE_RATIO = 0.2\n",
    "PARTITION_RANGE = 1000\n",
    "CACHE_PATH = \"./.replay_cache\"\n",
    "RASTER_RESOLUTION = np.array((7,7,5))\n",
    "RASTER_NCOLS = np.prod(RASTER_RESOLUTION)\n",
    "PLAY_AREA_DIMS = np.array((4096*2, 5120*2, 2044))\n",
    "PLAY_AREA_OFFSET = np.array((4096, 5120, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval (TODO)\n",
    "\n",
    "This stage retrieves replays from the internet, and caches them locally.\n",
    "\n",
    "\n",
    "### Train / Validation / Test Split (TODO)\n",
    "\n",
    "Data partitioning should be done on a per-replay basis, so that there are no chances for the model to memorize similar frames from the same replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching replays at: /home/axelmagn/.replays_cache\n"
     ]
    }
   ],
   "source": [
    "import data\n",
    "importlib.reload(data)\n",
    "data_manager = data.FileCachedCalculated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay(object):\n",
    "    def __init__(self, df, proto):\n",
    "        self.df = df\n",
    "        self.proto = proto\n",
    "\n",
    "def get_replay(replay_id, manager):\n",
    "    df = manager.get_pandas(replay_id)\n",
    "    proto = manager.get_proto(replay_id)\n",
    "    if df is None or proto is None:\n",
    "        return None\n",
    "    return Replay(df, proto)\n",
    "\n",
    "def partition_replays(replay_list):\n",
    "    return (\n",
    "        list(filter(_is_train_partition, replay_list)),\n",
    "        list(filter(_is_validate_partition, replay_list)),\n",
    "        list(filter(_is_test_partition, replay_list)),\n",
    "    )\n",
    "\n",
    "_TRAIN_PARTITION = (1 - TEST_RATIO - VALIDATE_RATIO) * PARTITION_RANGE\n",
    "_TEST_PARTITION = (1 - TEST_RATIO) * PARTITION_RANGE\n",
    "\n",
    "def _is_train_partition(replay_id):\n",
    "    id_value = abs(hash(replay_id)) % PARTITION_RANGE\n",
    "    return id_value < _TRAIN_PARTITION\n",
    "\n",
    "def _is_validate_partition(replay_id):\n",
    "    id_value = abs(hash(replay_id)) % PARTITION_RANGE\n",
    "    return id_value >= _TRAIN_PARTITION and id_value < _TEST_PARTITION\n",
    "\n",
    "def _is_test_partition(replay_id):\n",
    "    id_value = abs(hash(replay_id)) % PARTITION_RANGE\n",
    "    return id_value >= _TEST_PARTITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D4DE3A894229D5B8A5D0AE9091D3CA6C',\n",
       " '4DC852DA4D28F5D9047C509CA03412C8',\n",
       " 'EF578C404F791CE044EFDEB0036DA5EA',\n",
       " 'B828B7FD472EE7A19E9A9C8ECB7CA14B',\n",
       " 'A21C39DD402315E9831EC58C7A331F64']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now, we will just do a manual split of the first 10 replays\n",
    "replays = data_manager.get_replay_list(num=50)\n",
    "train_replays, validate_replays, test_replays = partition_replays(replays)\n",
    "replays[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 11, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_replays), len(validate_replays), len(test_replays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This stage takes replay dataframes and protobufs and prepares them as a numpy matrix suitable for model ingestion and analysis. There are 2 possible schemas I want to explore for state representation. \n",
    "\n",
    "\n",
    "### Schema: analog positions (DONE)\n",
    "\n",
    "Records shall be represented as position vectors as shown below\n",
    "\n",
    "```\n",
    "<ball> <t0_p0> <t0_p1> <t0_p2> <t1_p0> <t1_p1> <t1_p2>\n",
    "foreach: <pos_x> <pos_y> <pos_z>\n",
    "```\n",
    "\n",
    "### Schema: rasterized positions (TODO)\n",
    "\n",
    "Records shall be represented as 3-channel rasterized images, with a channel each for ball, team_0, and team_1.   This can be generated from the condensed positions.\n",
    "\n",
    "### Filtering (TODO)\n",
    "\n",
    "Starting positions are likely to be over-represented in this dataset.  It may be useful to filter out all frames before a ball has been hit.\n",
    "\n",
    "### Reordering (TODO)\n",
    "\n",
    "The data could then be expanded to capture the symmetries of team and player reordering.  Player ordering in the condensed schema is arbitrary, and each record could generate `3! * 3! = 36` permutations. Team ordering is not arbitrary, but teams could be swapped by negating `pos_y` values.  This would increase the total available permutations to `36 * 2 = 72`.  There is also x-axis symmetry, so by negating X values we get another mirror, bringing it to `72 * 2 = 144` permutations.\n",
    "\n",
    "### Distortion (TODO)\n",
    "\n",
    "Optionally, it would be possible to slightly randomize positions, in order to help generalize.\n",
    "\n",
    "\n",
    "### Dropout (TODO)\n",
    "\n",
    "The original plan was to omit one player, and make the predictor guess it. If we are already have generated all reorderings, it should be sufficient to simply omit the final player.  However, I would also like to explore the possibility of a GAN or Autoencoder where one position within a valid record is fabricated, and the predictor must reconstruct the valid position from the invalid one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analog_positions(replay):\n",
    "    assert replay is not None\n",
    "    proto = replay.proto\n",
    "    dataframe = replay.df\n",
    "    assert proto is not None\n",
    "    assert dataframe is not None\n",
    "    ball_posn = get_named_posn(dataframe, 'ball')\n",
    "    ball_posn = add_col_prefix(ball_posn, \"ball_\")\n",
    "    t0_posns = []\n",
    "    t1_posns = []\n",
    "    for player in proto.players:\n",
    "        posn = get_named_posn(dataframe, player.name)\n",
    "        if player.is_orange == 0:\n",
    "            t0_posns.append(posn)\n",
    "        else:\n",
    "            t1_posns.append(posn)\n",
    "    for i, posn in enumerate(t0_posns):\n",
    "        t0_posns[i] = add_col_prefix(posn, 't0_p{}_'.format(i))\n",
    "    for i, posn in enumerate(t1_posns):\n",
    "        t1_posns[i] = add_col_prefix(posn, 't1_p{}_'.format(i))\n",
    "    positions = [ball_posn] + t0_posns + t1_posns\n",
    "    return pd.concat(positions, axis=1).dropna()\n",
    "\n",
    "def get_named_posn(dataframe, name):\n",
    "    return dataframe[name][POSITION_COLUMNS]\n",
    "\n",
    "def get_named_player(proto, player_name):\n",
    "    for player in proto.players:\n",
    "        if player.name == player_name:\n",
    "            return player\n",
    "    raise ValueError(\"Could not find player: \" + player_name)\n",
    "    \n",
    "def add_col_prefix(dataframe, prefix):\n",
    "    return dataframe.rename(lambda col: prefix + col, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "E0603 21:00:02.017177 139964721129088 pandas_manager.py:29] Failure to read pandas [] from memory: Struct error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 49, in get_array\n",
      "    starting_byte = struct.unpack('i', chunk)[0]\n",
      "struct.error: unpack requires a buffer of 4 bytes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 27, in safe_read_pandas_to_memory\n",
      "    return PandasManager.read_numpy_from_memory(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 40, in read_numpy_from_memory\n",
      "    array = read_array_from_file(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 36, in read_array_from_file\n",
      "    numpy_array, num_bytes = get_array(file, chunk)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 51, in get_array\n",
      "    raise EOFError('Struct error')\n",
      "EOFError: Struct error\n",
      "E0603 21:00:04.166454 139964721129088 pandas_manager.py:29] Failure to read pandas [] from memory: Struct error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 49, in get_array\n",
      "    starting_byte = struct.unpack('i', chunk)[0]\n",
      "struct.error: unpack requires a buffer of 4 bytes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 27, in safe_read_pandas_to_memory\n",
      "    return PandasManager.read_numpy_from_memory(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 40, in read_numpy_from_memory\n",
      "    array = read_array_from_file(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 36, in read_array_from_file\n",
      "    numpy_array, num_bytes = get_array(file, chunk)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 51, in get_array\n",
      "    raise EOFError('Struct error')\n",
      "EOFError: Struct error\n",
      "E0603 21:00:04.885106 139964721129088 pandas_manager.py:29] Failure to read pandas [] from memory: Struct error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 49, in get_array\n",
      "    starting_byte = struct.unpack('i', chunk)[0]\n",
      "struct.error: unpack requires a buffer of 4 bytes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 27, in safe_read_pandas_to_memory\n",
      "    return PandasManager.read_numpy_from_memory(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 40, in read_numpy_from_memory\n",
      "    array = read_array_from_file(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 36, in read_array_from_file\n",
      "    numpy_array, num_bytes = get_array(file, chunk)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 51, in get_array\n",
      "    raise EOFError('Struct error')\n",
      "EOFError: Struct error\n",
      "E0603 21:00:05.916715 139964721129088 pandas_manager.py:29] Failure to read pandas [] from memory: Struct error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 49, in get_array\n",
      "    starting_byte = struct.unpack('i', chunk)[0]\n",
      "struct.error: unpack requires a buffer of 4 bytes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 27, in safe_read_pandas_to_memory\n",
      "    return PandasManager.read_numpy_from_memory(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 40, in read_numpy_from_memory\n",
      "    array = read_array_from_file(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 36, in read_array_from_file\n",
      "    numpy_array, num_bytes = get_array(file, chunk)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 51, in get_array\n",
      "    raise EOFError('Struct error')\n",
      "EOFError: Struct error\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ball_pos_x</th>\n",
       "      <th>ball_pos_y</th>\n",
       "      <th>ball_pos_z</th>\n",
       "      <th>t0_p0_pos_x</th>\n",
       "      <th>t0_p0_pos_y</th>\n",
       "      <th>t0_p0_pos_z</th>\n",
       "      <th>t0_p1_pos_x</th>\n",
       "      <th>t0_p1_pos_y</th>\n",
       "      <th>t0_p1_pos_z</th>\n",
       "      <th>t0_p2_pos_x</th>\n",
       "      <th>...</th>\n",
       "      <th>t0_p2_pos_z</th>\n",
       "      <th>t1_p0_pos_x</th>\n",
       "      <th>t1_p0_pos_y</th>\n",
       "      <th>t1_p0_pos_z</th>\n",
       "      <th>t1_p1_pos_x</th>\n",
       "      <th>t1_p1_pos_y</th>\n",
       "      <th>t1_p1_pos_z</th>\n",
       "      <th>t1_p2_pos_x</th>\n",
       "      <th>t1_p2_pos_y</th>\n",
       "      <th>t1_p2_pos_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>-2464.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>-3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1951.0</td>\n",
       "      <td>2465.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-255.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>-2464.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>-3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1951.0</td>\n",
       "      <td>2465.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-255.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>-2464.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>-3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1951.0</td>\n",
       "      <td>2465.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-255.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>-2464.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>-3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1951.0</td>\n",
       "      <td>2465.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-255.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>-2464.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-256.0</td>\n",
       "      <td>-3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-1951.0</td>\n",
       "      <td>2465.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>-255.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ball_pos_x  ball_pos_y  ball_pos_z  t0_p0_pos_x  t0_p0_pos_y  \\\n",
       "index                                                                 \n",
       "0             0.0         0.0        93.0       1952.0      -2464.0   \n",
       "1             0.0         0.0        93.0       1952.0      -2464.0   \n",
       "2             0.0         0.0        93.0       1952.0      -2464.0   \n",
       "3             0.0         0.0        93.0       1952.0      -2464.0   \n",
       "4             0.0         0.0        93.0       1952.0      -2464.0   \n",
       "\n",
       "       t0_p0_pos_z  t0_p1_pos_x  t0_p1_pos_y  t0_p1_pos_z  t0_p2_pos_x  ...  \\\n",
       "index                                                                   ...   \n",
       "0             18.0       -256.0      -3840.0         18.0        256.0  ...   \n",
       "1             18.0       -256.0      -3840.0         18.0        256.0  ...   \n",
       "2             18.0       -256.0      -3840.0         18.0        256.0  ...   \n",
       "3             18.0       -256.0      -3840.0         18.0        256.0  ...   \n",
       "4             18.0       -256.0      -3840.0         18.0        256.0  ...   \n",
       "\n",
       "       t0_p2_pos_z  t1_p0_pos_x  t1_p0_pos_y  t1_p0_pos_z  t1_p1_pos_x  \\\n",
       "index                                                                    \n",
       "0             18.0      -1951.0       2465.0         18.0        257.0   \n",
       "1             18.0      -1951.0       2465.0         18.0        257.0   \n",
       "2             18.0      -1951.0       2465.0         18.0        257.0   \n",
       "3             18.0      -1951.0       2465.0         18.0        257.0   \n",
       "4             18.0      -1951.0       2465.0         18.0        257.0   \n",
       "\n",
       "       t1_p1_pos_y  t1_p1_pos_z  t1_p2_pos_x  t1_p2_pos_y  t1_p2_pos_z  \n",
       "index                                                                   \n",
       "0           3840.0         18.0       -255.0       3840.0         18.0  \n",
       "1           3840.0         18.0       -255.0       3840.0         18.0  \n",
       "2           3840.0         18.0       -255.0       3840.0         18.0  \n",
       "3           3840.0         18.0       -255.0       3840.0         18.0  \n",
       "4           3840.0         18.0       -255.0       3840.0         18.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_replays = filter(lambda r: r is not None, (get_replay(replay_name, data_manager) for replay_name in train_replays))\n",
    "train_aps = pd.concat((analog_positions(r) for r in loaded_replays), axis=0)\n",
    "train_aps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0603 21:00:07.348217 139964721129088 pandas_manager.py:29] Failure to read pandas [] from memory: Struct error\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 49, in get_array\n",
      "    starting_byte = struct.unpack('i', chunk)[0]\n",
      "struct.error: unpack requires a buffer of 4 bytes\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 27, in safe_read_pandas_to_memory\n",
      "    return PandasManager.read_numpy_from_memory(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/pandas_manager.py\", line 40, in read_numpy_from_memory\n",
      "    array = read_array_from_file(buffer)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 36, in read_array_from_file\n",
      "    numpy_array, num_bytes = get_array(file, chunk)\n",
      "  File \"/home/axelmagn/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/carball/analysis/utils/numpy_manager.py\", line 51, in get_array\n",
      "    raise EOFError('Struct error')\n",
      "EOFError: Struct error\n"
     ]
    }
   ],
   "source": [
    "loaded_replays = filter(lambda r: r is not None, (get_replay(replay_name, data_manager) for replay_name in validate_replays))\n",
    "validate_aps = pd.concat((analog_positions(r) for r in loaded_replays), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and targets\n",
    "def get_analog_features(analog_positions_dataframe):\n",
    "    return np.copy(analog_positions_dataframe.values[:,:-NCOLS])\n",
    "\n",
    "def get_analog_targets(analog_positions_dataframe):\n",
    "    return np.copy(analog_positions_dataframe.values[:,-NCOLS:])\n",
    "\n",
    "# TODO: split train, validation, and test sets based on multiple replays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234458, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_aps_features = get_analog_features(train_aps)\n",
    "train_aps_targets = get_analog_targets(train_aps)\n",
    "validate_aps_features = get_analog_features(validate_aps)\n",
    "validate_aps_targets = get_analog_targets(validate_aps)\n",
    "train_aps_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rasterization.  \n",
    "\n",
    "def rasterize_row(aps):\n",
    "    \"\"\"\n",
    "    convert xyz coordinates for a single row into an image where pixels reflect number \n",
    "    of entities in that pixel's cell, normalized between 0 and 1.\n",
    "    \n",
    "    input schema: (position_columns,)\n",
    "    output schema (raster_x, raster_y, raster_z)\n",
    "    \"\"\"\n",
    "    n_entities = math.floor(aps.shape[0] / NCOLS)\n",
    "    aps = aps.reshape((n_entities, NCOLS))\n",
    "    aps = np.add(aps, PLAY_AREA_OFFSET)\n",
    "    aps = np.divide(aps, PLAY_AREA_DIMS)\n",
    "    aps = np.multiply(aps, RASTER_RESOLUTION)\n",
    "    aps = np.apply_along_axis(rasterize_pos, 1, aps)\n",
    "    aps = np.sum(aps, axis=0)\n",
    "    return aps\n",
    "  \n",
    "def rasterize_pos(position):\n",
    "    \"\"\"\n",
    "    convert a single xyz position into a rasterized image\n",
    "    \"\"\"\n",
    "    out = np.zeros(RASTER_RESOLUTION)\n",
    "    position = np.floor(position)\n",
    "    (x, y, z) = map(int, tuple(position))\n",
    "    out[x,y,z] = 1\n",
    "    return out\n",
    "\n",
    "def rasterize_batch(aps):\n",
    "    return np.apply_along_axis(rasterize_row, 1, aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-105da21135a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m a1 = np.array([[1,2,3,4,5,6],\n\u001b[1;32m      3\u001b[0m            [9,8,7,6,5,4]])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrasterize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_aps_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-447d22712bf9>\u001b[0m in \u001b[0;36mrasterize_batch\u001b[0;34m(aps)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrasterize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrasterize_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-447d22712bf9>\u001b[0m in \u001b[0;36mrasterize_row\u001b[0;34m(aps)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPLAY_AREA_DIMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRASTER_RESOLUTION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrasterize_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/personal/ReplayModels/venv/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[0;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mbuff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-447d22712bf9>\u001b[0m in \u001b[0;36mrasterize_pos\u001b[0;34m(position)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for axis 1 with size 7"
     ]
    }
   ],
   "source": [
    "# train_rps_features = rasterize_batch(train_aps_features)\n",
    "a1 = np.array([[1,2,3,4,5,6],\n",
    "           [9,8,7,6,5,4]])\n",
    "rasterize_batch(train_aps_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This stage evaluates different models on the training data.  Below is a list of model hypotheses that should be considered.\n",
    "\n",
    "### Models\n",
    "\n",
    "#### XGBoost (Baseline) (In Progress)\n",
    "\n",
    "XGBoost performs well at many complex regression problems, and is easy to apply using a library.  It will serve as the baseline against which more sophisticated model hypotheses can be compared.\n",
    "\n",
    "#### Clustering / KNN (Baseline) (TODO)\n",
    "\n",
    "Another simple solution would be to retrieve similar memorized positions.\n",
    "\n",
    "#### Deep Neural Networks (TODO)\n",
    "\n",
    "After baselines have been measured, more sophisticated models such as convolutional neural networks can be considered.\n",
    "\n",
    "### Selection Criteria\n",
    "\n",
    "As important as the models is our criteria for evaluating them.  Since this problem does not easily map to a real-world loss function, this can be somewhat arbitrary, and may depend on the output schema.\n",
    "\n",
    "#### RMSE (Single Output Schema)\n",
    "\n",
    "Old reliable.\n",
    "\n",
    "#### MAE (Single Output Schema)\n",
    "\n",
    "It may be justifiable to use this since I don't care too much about missing outliers.\n",
    "\n",
    "#### R^2 (Single Output Schema)\n",
    "\n",
    "SKLearn default.  Seems to have some advantages.  Will probably stick with this out of convenience.\n",
    "\n",
    "#### Probability Weighted Score (Distribution Output Schema)\n",
    "\n",
    "If the model outputs a probability distribution across multiple possible positions, I think it makes sense to add up the score of those positions, multiplied by their predicted probability weight.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Try training with XGBoost.  Key questions:\n",
    "\n",
    "1. would we benefit from more training data?\n",
    "2. would we benefit from more iterations?\n",
    "3. Are the hyperparameters optimaly tuned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(**kwargs):\n",
    "    return MultiOutputRegressor(xgb.XGBRegressor(learning_rate=0.01, objective='reg:squarederror', random_seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputRegressor(estimator=XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=1, gamma=0,\n",
       "                                            importance_type='gain',\n",
       "                                            learning_rate=0.01,\n",
       "                                            max_delta_step=0, max_depth=3,\n",
       "                                            min_child_weight=1, missing=None,\n",
       "                                            n_estimators=100, n_jobs=1,\n",
       "                                            nthread=None,\n",
       "                                            objective='reg:squarederror',\n",
       "                                            random_seed=0, random_state=0,\n",
       "                                            reg_alpha=0, reg_lambda=1,\n",
       "                                            scale_pos_weight=1, seed=None,\n",
       "                                            silent=None, subsample=1,\n",
       "                                            verbosity=1),\n",
       "                     n_jobs=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb_model()\n",
    "model.fit(train_features, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23961912393954052"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how does this model perform out of the box?\n",
    "model.score(validate_features, validate_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: give a shit about the other questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Dense Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TMP: Tensorflow sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10, activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  predictions = model(images)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1341739147901535, Accuracy: 95.8933334350586, Test Loss: 0.05695280432701111, Test Accuracy: 98.08999633789062\n",
      "Epoch 2, Loss: 0.0873652845621109, Accuracy: 97.33583068847656, Test Loss: 0.05193914473056793, Test Accuracy: 98.22000122070312\n",
      "Epoch 3, Loss: 0.06430377066135406, Accuracy: 98.03722381591797, Test Loss: 0.05261257290840149, Test Accuracy: 98.26333618164062\n",
      "Epoch 4, Loss: 0.05145478621125221, Accuracy: 98.4254150390625, Test Loss: 0.053377918899059296, Test Accuracy: 98.32250213623047\n",
      "Epoch 5, Loss: 0.042890507727861404, Accuracy: 98.68299865722656, Test Loss: 0.054339755326509476, Test Accuracy: 98.35399627685547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(),\n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(),\n",
    "                         test_accuracy.result()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
